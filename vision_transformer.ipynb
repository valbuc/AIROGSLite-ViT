{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, AdamW\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import (Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomVerticalFlip,\n",
    "                                    GaussianBlur,\n",
    "                                    RandomRotation, #  rotation\n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "NUM_WORKERS = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_file</th>\n",
       "      <th>new_file</th>\n",
       "      <th>delta_x</th>\n",
       "      <th>delta_y</th>\n",
       "      <th>orig_crop_side</th>\n",
       "      <th>side</th>\n",
       "      <th>scaling</th>\n",
       "      <th>orig_file_number</th>\n",
       "      <th>shuf_file_number</th>\n",
       "      <th>labels_string</th>\n",
       "      <th>labels_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8848</th>\n",
       "      <td>DEV13781.jpg</td>\n",
       "      <td>SHUF00000</td>\n",
       "      <td>176</td>\n",
       "      <td>-144</td>\n",
       "      <td>2272</td>\n",
       "      <td>2272</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13781</td>\n",
       "      <td>0</td>\n",
       "      <td>NRG</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>DEV12858.jpg</td>\n",
       "      <td>SHUF00001</td>\n",
       "      <td>-89</td>\n",
       "      <td>-904</td>\n",
       "      <td>3150</td>\n",
       "      <td>3150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12858</td>\n",
       "      <td>1</td>\n",
       "      <td>NRG</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13185</th>\n",
       "      <td>DEV02899.jpg</td>\n",
       "      <td>SHUF00002</td>\n",
       "      <td>-59</td>\n",
       "      <td>-685</td>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2899</td>\n",
       "      <td>2</td>\n",
       "      <td>NRG</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6229</th>\n",
       "      <td>DEV12129.jpg</td>\n",
       "      <td>SHUF00003</td>\n",
       "      <td>168</td>\n",
       "      <td>-152</td>\n",
       "      <td>2256</td>\n",
       "      <td>2256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12129</td>\n",
       "      <td>3</td>\n",
       "      <td>NRG</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12359</th>\n",
       "      <td>DEV04490.jpg</td>\n",
       "      <td>SHUF00004</td>\n",
       "      <td>-66</td>\n",
       "      <td>-886</td>\n",
       "      <td>3166</td>\n",
       "      <td>3166</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4490</td>\n",
       "      <td>4</td>\n",
       "      <td>NRG</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11147</th>\n",
       "      <td>DEV04506.jpg</td>\n",
       "      <td>SHUF14995</td>\n",
       "      <td>-44</td>\n",
       "      <td>-905</td>\n",
       "      <td>3175</td>\n",
       "      <td>3175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4506</td>\n",
       "      <td>14995</td>\n",
       "      <td>NRG</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>DEV12149.jpg</td>\n",
       "      <td>SHUF14996</td>\n",
       "      <td>-45</td>\n",
       "      <td>-857</td>\n",
       "      <td>3207</td>\n",
       "      <td>3207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12149</td>\n",
       "      <td>14996</td>\n",
       "      <td>RG</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8920</th>\n",
       "      <td>DEV00409.jpg</td>\n",
       "      <td>SHUF14997</td>\n",
       "      <td>-95</td>\n",
       "      <td>-870</td>\n",
       "      <td>2987</td>\n",
       "      <td>2987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>409</td>\n",
       "      <td>14997</td>\n",
       "      <td>RG</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030</th>\n",
       "      <td>DEV01824.jpg</td>\n",
       "      <td>SHUF14998</td>\n",
       "      <td>197</td>\n",
       "      <td>-127</td>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1824</td>\n",
       "      <td>14998</td>\n",
       "      <td>NRG</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>DEV10476.jpg</td>\n",
       "      <td>SHUF14999</td>\n",
       "      <td>176</td>\n",
       "      <td>-144</td>\n",
       "      <td>2272</td>\n",
       "      <td>2272</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10476</td>\n",
       "      <td>14999</td>\n",
       "      <td>NRG</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          orig_file   new_file  delta_x  delta_y  orig_crop_side  side  \\\n",
       "8848   DEV13781.jpg  SHUF00000      176     -144            2272  2272   \n",
       "1097   DEV12858.jpg  SHUF00001      -89     -904            3150  3150   \n",
       "13185  DEV02899.jpg  SHUF00002      -59     -685            2486  2486   \n",
       "6229   DEV12129.jpg  SHUF00003      168     -152            2256  2256   \n",
       "12359  DEV04490.jpg  SHUF00004      -66     -886            3166  3166   \n",
       "...             ...        ...      ...      ...             ...   ...   \n",
       "11147  DEV04506.jpg  SHUF14995      -44     -905            3175  3175   \n",
       "5655   DEV12149.jpg  SHUF14996      -45     -857            3207  3207   \n",
       "8920   DEV00409.jpg  SHUF14997      -95     -870            2987  2987   \n",
       "4030   DEV01824.jpg  SHUF14998      197     -127            2338  2338   \n",
       "2004   DEV10476.jpg  SHUF14999      176     -144            2272  2272   \n",
       "\n",
       "       scaling  orig_file_number  shuf_file_number labels_string  labels_int  \n",
       "8848       1.0             13781                 0           NRG         0.0  \n",
       "1097       1.0             12858                 1           NRG         0.0  \n",
       "13185      1.0              2899                 2           NRG         0.0  \n",
       "6229       1.0             12129                 3           NRG         0.0  \n",
       "12359      1.0              4490                 4           NRG         0.0  \n",
       "...        ...               ...               ...           ...         ...  \n",
       "11147      1.0              4506             14995           NRG         0.0  \n",
       "5655       1.0             12149             14996            RG         1.0  \n",
       "8920       1.0               409             14997            RG         1.0  \n",
       "4030       1.0              1824             14998           NRG         0.0  \n",
       "2004       1.0             10476             14999           NRG         0.0  \n",
       "\n",
       "[15000 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch32-384\")\n",
    "labels = pd.read_csv(\"data/img_info.csv\", index_col=0)\n",
    "labels.sort_values(\"shuf_file_number\", inplace=True)\n",
    "\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 474.40it/s]\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "for filename in tqdm(os.listdir(\"data/shuffled_square_75\")[:300]):\n",
    "    file = f\"data/shuffled_square_75/{filename}\"\n",
    "    img = Image.open(file)\n",
    "    images.append(img)\n",
    "\n",
    "images_train = images[:100]\n",
    "images_val = images[100:200]\n",
    "images_test = images[200:]\n",
    "\n",
    "labels_list = labels.labels_int.to_list()\n",
    "labels_train = labels_list[:100]\n",
    "labels_val = labels_list[100:200]\n",
    "labels_test = labels_list[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            RandomHorizontalFlip(),\n",
    "            RandomVerticalFlip(),\n",
    "            RandomRotation(45),\n",
    "            GaussianBlur(5, (0.1, 0.2)),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(images):\n",
    "    return [_train_transforms(img) for img in tqdm(images)]\n",
    "\n",
    "def val_transforms(images):\n",
    "    return [_val_transforms(img) for img in tqdm(images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:33<00:00,  3.00it/s]\n",
      "100%|██████████| 100/100 [01:28<00:00,  1.12it/s]\n",
      "100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "train_transformed = train_transforms(images_train)\n",
    "val_transformed = val_transforms(images_val)\n",
    "test_transformed = val_transforms(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]]),\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = [{\"pixel_values\": train_transformed[i], \"label\": int(labels_train[i])} for i in range(100)]\n",
    "val_ds = [{\"pixel_values\": val_transformed[i], \"label\": int(labels_val[i])} for i in range(100)]\n",
    "test_ds = [{\"pixel_values\": test_transformed[i], \"label\": int(labels_test[i])} for i in range(100)]\n",
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(images):\n",
    "    pixel_values = torch.stack([img[\"pixel_values\"] for img in images])\n",
    "    labels = torch.LongTensor([img[\"label\"] for img in images])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_batch_size = 2\n",
    "eval_batch_size = 2\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, shuffle=True, collate_fn=collate_fn, batch_size=train_batch_size, num_workers = NUM_WORKERS)\n",
    "val_dataloader = DataLoader(val_ds, collate_fn=collate_fn, batch_size=eval_batch_size)\n",
    "test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 14596, 17012, 13608, 9468, 10076, 11592, 19944) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1164\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1324\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1325\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1326\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1327\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1176\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1175\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1176\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   1178\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 14596, 17012, 13608, 9468, 10076, 11592, 19944) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NRG\", 1:\"RG\"}\n",
    "label2id = {\"NRG\": 0, \"RG\": 1}\n",
    "\n",
    "class ViTLightningModule(pl.LightningModule):\n",
    "    def __init__(self, num_labels=10):\n",
    "        super(ViTLightningModule, self).__init__()\n",
    "        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch32-384',\n",
    "                                                              num_labels=2,\n",
    "                                                              id2label=id2label,\n",
    "                                                              label2id=label2id,\n",
    "                                                              ignore_mismatched_sizes=True)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        return outputs.logits\n",
    "        \n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        logits = self(pixel_values)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(logits, labels)\n",
    "        predictions = logits.argmax(-1)\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        accuracy = correct/pixel_values.shape[0]\n",
    "\n",
    "        return loss, accuracy\n",
    "      \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        self.log(\"training_accuracy\", accuracy)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n",
    "        # not require weight_decay but just using AdamW out-of-the-box works fine\n",
    "        return AdamW(self.parameters(), lr=5e-5)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch32-384 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:89: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: c:\\Users\\Valen\\Documents\\Master AI VU\\Medical Imaging\\AIROGSLite-AI4MI-VU-2022\\lightning_logs\n",
      "c:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name | Type                      | Params\n",
      "---------------------------------------------------\n",
      "0 | vit  | ViTForImageClassification | 87.5 M\n",
      "---------------------------------------------------\n",
      "87.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.5 M    Total params\n",
      "350.119   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e6d59d69d5434aaeb54c39853be3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Valen\\Anaconda3\\envs\\airogs\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31375c0eb48349be968e6b35cabecdba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc164bc8869416a84b9835bfcae682a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f15a26edc14b5fb0e5f63a376030eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e5a6fd480040969fd82f9802586946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff0d6292b45481a84212e9b1bbf3133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# for early stopping, see https://pytorch-lightning.readthedocs.io/en/1.0.0/early_stopping.html?highlight=early%20stopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    strict=False,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "model = ViTLightningModule()\n",
    "trainer = Trainer(gpus=1, callbacks=[EarlyStopping(monitor='validation_loss')], max_epochs=30)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('airogs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "061bd6094ad0c8a3908ed2966c1aa3f09fb3cf99cdbbdc5979d65ee9ea258955"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
